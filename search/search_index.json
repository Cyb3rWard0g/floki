{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Floki: Agentic Workflows Made Simple","text":"<p>Floki is an open-source framework for researchers and developers to experiment with LLM-based autonomous agents. It provides tools to create, orchestrate, and manage agents while seamlessly connecting to LLM inference APIs. Built on Dapr, Floki leverages a unified programming model that simplifies microservices and supports both deterministic workflows and event-driven interactions. Using Dapr\u2019s Virtual Actor pattern, Floki enables agents to function as independent, self-contained units that process messages sequentially, eliminating concurrency concerns while seamlessly integrating into larger workflows. It also facilitates agent collaboration through Dapr\u2019s Pub/Sub integration, where agents communicate via a shared message bus, simplifying the design of workflows where tasks are distributed efficiently, and agents work together to achieve shared goals. By bringing together these features, Floki provides a powerful way to explore agentic workflows and the components that enable multi-agent systems to collaborate and scale, all powered by Dapr.</p>"},{"location":"#why-dapr","title":"Why Dapr \ud83c\udfa9?","text":"<p>Dapr provides Floki with a unified programming model that simplifies the development of resilient and scalable systems by offering built-in APIs for features such as service invocation, Pub/Sub messaging, workflows, and even state management. These components, essential for defining agentic workflows, allow developers to focus on designing agents and workflows rather than rebuilding foundational features. By leveraging Dapr\u2019s sidecar architecture and portable, event-driven runtime, Floki also enables agents to collaborate effectively, share tasks, and adapt dynamically across cloud and edge environments. This seamless integration brings together deterministic workflows and LLM-based decision-making into a unified system, making it easier to experiment with multi-agent systems and scalable agentic workflows.</p>"},{"location":"#key-dapr-features-in-floki","title":"Key Dapr Features in Floki:","text":"<ul> <li>\ud83c\udfaf Service-to-Service Invocation: Facilitates direct communication between agents with built-in service discovery, error handling, and distributed tracing. Agents can leverage this for synchronous messaging in multi-agent workflows.</li> <li>\u26a1\ufe0f Publish and Subscribe: Supports loosely coupled collaboration between agents through a shared message bus. This enables real-time, event-driven interactions critical for task distribution and coordination.</li> <li>\ud83d\udd04 Workflow API: Defines long-running, persistent workflows that combine deterministic processes with LLM-based decision-making. Floki uses this to orchestrate complex multi-step agentic workflows seamlessly.</li> <li>\ud83e\udde0 State Management: Provides a flexible key-value store for agents to retain context across interactions, ensuring continuity and adaptability during workflows.</li> <li>\ud83e\udd16 Actors: Implements the Virtual Actor pattern, allowing agents to operate as self-contained, stateful units that handle messages sequentially. This eliminates concurrency concerns and enhances scalability in Floki's agent systems.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Set up in 2 minutes</p> <p>Install <code>Floki</code> with <code>pip</code> and set up your dapr environment in minutes</p> <p> Installation</p> </li> <li> <p> Start experimenting</p> <p>Build your first agent and design a custom workflow to get started with Floki.</p> <p> Quickstarts</p> </li> <li> <p> Learn more</p> <p>Learn more about Floki and its main components!</p> <p> Concepts</p> </li> <li> <p> Open Source, MIT</p> <p>Floki is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-floki","title":"Install Floki","text":"<p>Info</p> <p>make sure you have Python already installed. <code>Python &gt;=3.9</code></p>"},{"location":"installation/#as-a-python-package-using-pip","title":"As a Python package using Pip","text":"<pre><code>pip install floki-ai\n</code></pre>"},{"location":"installation/#remotely-from-github","title":"Remotely from GitHub","text":"<pre><code>pip install git+https://github.com/Cyb3rWard0g/floki.git\n</code></pre>"},{"location":"installation/#from-source-with-poetry","title":"From source with <code>poetry</code>:","text":"<pre><code>git clone https://github.com/Cyb3rWard0g/floki\n\ncd floki\n\npoetry install\n</code></pre>"},{"location":"installation/#install-dapr-cli","title":"Install Dapr CLI","text":"<p>Install the Dapr CLI to manage Dapr-related tasks like running applications with sidecars, viewing logs, and launching the Dapr dashboard. It works seamlessly with both self-hosted and Kubernetes environments. For a complete step-by-step guide, visit the official Dapr CLI installation page.</p> <p>Verify the CLI is installed by restarting your terminal/command prompt and running the following:</p> <pre><code>dapr -h\n</code></pre>"},{"location":"installation/#initialize-dapr-in-local-mode","title":"Initialize Dapr in Local Mode","text":"<p>Info</p> <p>Make sure you have Docker already installed. I use Docker Desktop.</p> <p>Initialize Dapr locally to set up a self-hosted environment for development. This process installs Dapr sidecar binaries, runs essential services like Redis (state store and message broker) and Zipkin (observability), and prepares a default components folder. For detailed steps, see the official guide on initializing Dapr locally.</p> <p>To initialize the Dapr control plane containers and create a default configuration file, run:</p> <pre><code>dapr init\n</code></pre> <p>Verify you have container instances with <code>daprio/dapr</code>, <code>openzipkin/zipkin</code>, and <code>redis</code> images running:</p> <pre><code>docker ps\n</code></pre>"},{"location":"quickstarts/","title":"Floki Quickstarts","text":"<p>Dive into our Floki quickstarts to explore core features with practical code samples, designed to get you up and running quickly. From setup to hands-on examples, these resources are your first step into the world of Floki.</p> <p>Info</p> <p>Not all quickstarts require Docker, but it is recommended to have your local Dapr environment set up with Docker for the best development experience and to follow the steps in this guide seamlessly.</p>"},{"location":"quickstarts/#quickstarts","title":"Quickstarts","text":"Scenario Description LLM Inference Client Learn how to set up and use Floki's LLM Inference Client to interact with language models like OpenAI's <code>gpt-4o</code>. This quickstart covers initializing the OpenAIChatClient, managing environment variables, and generating structured responses using Pydantic models. LLM-based AI Agents Discover how to create LLM-based autonomous agents. This quickstart walks you through defining tools with Pydantic schemas, setting up agents with clear roles and goals, and enabling dynamic task execution using OpenAI's Function Calling. Dapr &amp; Floki Workflows Explore how Floki builds on Dapr workflows to simplify long-running process management. Learn how to define tasks, integrate tools, and add LLM reasoning to extend workflow capabilities. LLM-based Task Workflows Design structured, step-by-step workflows with LLMs providing reasoning at key stages. This quickstart covers task orchestration with Python functions and integrating LLM Inference APIs. Event-Driven Agentic Workflows Leverage event-driven systems with pub/sub messaging to enable agents to collaborate dynamically. This quickstart demonstrates setting up workflows for decentralized, real-time agent interaction."},{"location":"quickstarts/agentic_workflows/","title":"Event-Driven Agentic Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>Event-Driven Agentic Workflows in <code>Floki</code> take advantage of an event-driven system using pub/sub messaging and a shared message bus. Agents operate as autonomous entities that respond to events dynamically, enabling real-time interactions and collaboration. These workflows are highly adaptable, allowing agents to communicate, share tasks, and reason through events triggered by their environment. This approach is best suited for decentralized systems requiring dynamic agent collaboration across distributed applications.</p>"},{"location":"quickstarts/agentic_workflows/#agents-as-services","title":"Agents as Services","text":"<p>In <code>Floki</code>, agents can be exposed as services, making them reusable, modular, and easy to integrate into event-driven workflows. Each agent runs as a microservice, wrapped in a Dapr-enabled FastAPI server. This design allows agents to operate independently while communicating through Dapr\u2019s pub/sub messaging and interacting with state stores or other services.</p> <p>The way to structure such a project is straightforward. We organize our services into a directory that contains individual folders for each agent, along with a components/ directory for Dapr configurations. Each agent service includes its own app.py file, where the FastAPI server and the agent logic are defined.</p> <pre><code>components/                # Dapr configuration files\n\u251c\u2500\u2500 statestore.yaml        # State store configuration\n\u251c\u2500\u2500 pubsub.yaml            # Pub/Sub configuration\n\u2514\u2500\u2500 ...                    # Other Dapr components\nservices/                  # Directory for agent services\n\u251c\u2500\u2500 agent1/                # First agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent1\n\u2502   \u2514\u2500\u2500 ...                # Additional agent1 files\n\u2502\u2500\u2500 agent2/                # Second agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent2\n\u2502   \u2514\u2500\u2500 ...                # Additional agent2 files\n\u2514\u2500\u2500 ...                    # More agents\n</code></pre>"},{"location":"quickstarts/agentic_workflows/#your-first-service","title":"Your First Service","text":"<p>Let's start by definining a <code>Hobbit</code> service with a specific <code>name</code>, <code>role</code>, <code>goal</code> and <code>instructions</code>.</p> <pre><code>services/                  # Directory for agent services\n\u251c\u2500\u2500 hobbit/                # Hobbit Service\n\u2502   \u251c\u2500\u2500 app.py             # Dapr Enabled FastAPI app for Hobbit\n</code></pre> <p>Create the <code>app.py</code> script and provide the following information.</p> <pre><code>from floki import Agent, AgentService\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        # Define Agent\n        hobbit_agent = Agent(\n            role=\"Hobbit\",\n            name=\"Frodo\",\n            goal=\"Take the ring to Mordor\",\n            instructions=[\"Speak like Frodo\"]\n        )\n        # Expose Agent as a Service\n        hobbit_service = AgentService(\n            agent=hobbit_agent,\n            message_bus_name=\"messagepubsub\",\n            agents_state_store_name=\"agentstatestore\",\n            port=8001,\n            daprGrpcPort=50001\n        )\n        await hobbit_service.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>Now, you can define multiple services following this format, but it's essential to pay attention to key areas to ensure everything runs smoothly. Specifically, focus on correctly configuring the components (e.g., <code>statestore</code> and <code>pubsub</code> names) and incrementing the ports for each service.</p> <p>Key Considerations:</p> <ul> <li>Ensure the <code>message_bus_name</code> matches the <code>pub/sub</code> component name in your <code>pubsub.yaml</code> file.</li> <li>Verify the <code>agents_state_store_name</code> matches the state store component defined in your <code>statestore.yaml</code> file.</li> <li>Increment the port for each new agent service (e.g., 8001, 8002, 8003).</li> <li>Similarly, increment the <code>daprGrpcPort</code> for each service (e.g., 50001, 50002, 50003) to avoid conflicts.</li> <li>Customize the Agent parameters (<code>role</code>, <code>name</code>, <code>goal</code>, and <code>instructions</code>) to match the behavior you want for each service.</li> </ul>"},{"location":"quickstarts/agentic_workflows/#the-agentic-workflow-service","title":"The Agentic Workflow Service","text":"<p>The Agentic Workflow Service in Floki extends workflows to orchestrate communication among agents. It allows you to send messages to agents to trigger their participation and monitors a shared message bus to listen for all messages being passed. This enables dynamic collaboration and task distribution among agents.</p> <p>Types of Agentic Workflows:</p> <ul> <li>Random: Distributes tasks to agents randomly, ensuring a non-deterministic selection of participating agents for each task.</li> <li>RoundRobin: Cycles through agents in a fixed order, ensuring each agent has an equal opportunity to participate in tasks.</li> <li>LLM-based: Leverages an LLM to decide which agent to trigger based on the content and context of the task and chat history.</li> </ul> <p>Next, we\u2019ll define a <code>Random Agentic Workflow Service</code> to demonstrate how this concept can be implemented.</p> <pre><code>from floki import RandomWorkflowService\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        random_workflow_service = RandomWorkflowService(\n            name=\"Orchestrator\",\n            message_bus_name=\"messagepubsub\",\n            agents_state_store_name=\"agentstatestore\",\n            workflow_state_store_name=\"workflowstatestore\",\n            port=8004,\n            daprGrpcPort=50004,\n            max_iterations=2\n        )\n\n        await random_workflow_service.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>Unlike <code>Agents as Services</code>, the <code>Agentic Workflow Service</code> does not require an agent parameter since it orchestrates communication among multiple agents rather than representing a single agent. Instead, the configuration focuses on workflow-specific parameters:</p> <ul> <li>Max Iterations: Defines the maximum number of iterations the workflow will perform, ensuring controlled task execution and preventing infinite loops.</li> <li>Workflow State Store Name: Specifies the state store used to persist the workflow\u2019s state, allowing for reliable recovery and tracking of workflow progress.</li> </ul> <p>These differences reflect the distinct purpose of the Agentic Workflow Service, which acts as a centralized orchestrator rather than an individual agent service.</p>"},{"location":"quickstarts/agentic_workflows/#the-multi-app-run-template-file","title":"The Multi-App Run template file","text":"<p>The Multi-App Run Template File is a YAML configuration file named <code>dapr.yaml</code> that allows you to run multiple applications simultaneously. This file is placed at the same level as the <code>components/</code> and <code>services/</code> directories, ensuring a consistent and organized project structure.</p> <pre><code>dapr.yaml                  # The Multi-App Run template\ncomponents/                # Dapr configuration files\n\u251c\u2500\u2500 statestore.yaml        # State store configuration\n\u251c\u2500\u2500 pubsub.yaml            # Pub/Sub configuration\n\u2514\u2500\u2500 ...                    # Other Dapr components\nservices/                  # Directory for agent services\n\u251c\u2500\u2500 agent1/                # First agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent1\n\u2502   \u2514\u2500\u2500 ...                # Additional agent1 files\n\u2502\u2500\u2500 agent2/                # Second agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent2\n\u2502   \u2514\u2500\u2500 ...                # Additional agent2 files\n\u2514\u2500\u2500 ...                    # More agents\n</code></pre> <p>Following our current scenario, we can set the following <code>Multi-App Run</code> template file:</p> <pre><code># https://docs.dapr.io/developing-applications/local-development/multi-app-dapr-run/multi-app-template/#template-properties\nversion: 1\ncommon:\n  resourcesPath: ./components\n  logLevel: info\n  appLogDestination: console\n  daprdLogDestination: console\n\napps:\n- appId: HobbitApp\n  appDirPath: ./services/hobbit/\n  appPort: 8001\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50001\n\n- appId: WizardApp\n  appDirPath: ./services/wizard/\n  appPort: 8002\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50002\n\n- appId: ElfApp\n  appDirPath: ./services/elf/\n  appPort: 8003\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50003\n\n- appId: WorkflowApp\n  appDirPath: ./services/workflow-random/\n  appPort: 8004\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50004\n</code></pre>"},{"location":"quickstarts/agentic_workflows/#starting-all-service-servers","title":"Starting All Service Servers","text":"<p>Tip</p> <p>Make sure you have your environment variables set up in an <code>.env</code> file so that the library can pick it up and use it to communicate with <code>OpenAI</code> services. We set them up in the LLM Inference Client section</p> <p>To start all the service servers defined in your project, you can use the Dapr CLI with the Multi-App Run template file. When you provide a directory path, the CLI will look for the dapr.yaml file (the default name for the template) in that directory. If the file is not found, the CLI will return an error.</p> <p>To execute the command, ensure you are in the root directory where the dapr.yaml file is located, then run:</p> <pre><code>dapr run -f .\n</code></pre> <p>This command reads the dapr.yaml file and starts all the services specified in the template.</p>"},{"location":"quickstarts/agentic_workflows/#start-workflow-via-an-http-request","title":"Start Workflow via an HTTP Request","text":"<p>Once all services are running, you can initiate the workflow by making an HTTP POST request to the Agentic Workflow Service. This service orchestrates the workflow, triggering agent actions and handling communication among agents.</p> <p>Here\u2019s an example of how to start the workflow using <code>curl</code>:</p> <pre><code>curl -i -X POST http://localhost:8004/RunWorkflow \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"message\": \"How to get to Mordor? Let's all help!\"}'\n</code></pre> <p>In this example:</p> <ul> <li>The request is sent to the Agentic Workflow Service running on port 8004.</li> <li>The message parameter is passed as input to the workflow, which the agents will process.</li> <li>This command demonstrates how to interact with the Agentic Workflow Service to kick off a new workflow.</li> </ul> <p>If you check the console where you started all the service servers, you will see the following output once the system reaches its maximum number of iterations.</p> <p></p>"},{"location":"quickstarts/agentic_workflows/#monitor-workflow-execution","title":"Monitor Workflow Execution","text":"<p>As mentioned earlier, when we ran dapr init, Dapr initialized, a <code>Zipkin</code> container instance, used for observability and tracing. We can use the Zipkin container to monitor our workflow execution. To do this, open your browser and go to <code>http://localhost:9411/zipkin/</code>. From there:</p> <p>Click on <code>Find a Trace</code> and then <code>Run Query</code> to search for traces.</p> <p></p> <p>Select the trace entry with multiple spans labeled <code>&lt;workflow name&gt;: /taskhubsidecarservice/startinstance.</code></p> <p>When you open this entry, you\u2019ll see details about how each task or activity in the workflow was executed. If any task failed, the error will also be visible here.</p> <p></p>"},{"location":"quickstarts/agents/","title":"LLM-based AI Agents","text":"<p>In the <code>Floki</code> framework, agents are autonomous systems powered by large language models (LLMs) that serve as their reasoning engine. These agents use the LLM\u2019s parametric knowledge to process information, reason in natural language, and interact dynamically with their environment by leveraging tools. Tools allow the agents to perform real-world tasks, gather new information, and adapt their reasoning based on feedback.</p> <p>Info</p> <p>By default, <code>Floki</code> sets the agentic pattern for the <code>Agent</code> class to <code>toolcalling</code> mode, enabling AI agents to interact dynamically with external tools using OpenAI's Function Calling.</p> <p><code>Tool Calling</code> empowers agents to identify the right tools for a task, format the necessary arguments, and execute the tools independently. The results are then passed back to the LLM for further processing, enabling seamless and adaptive agent workflows.</p>"},{"location":"quickstarts/agents/#environment-variables","title":"Environment Variables","text":"<p>Create an <code>.env</code> file for your API keys and other environment variables with sensitive information that you do not want to hardcode.</p> <pre><code>OPENAI_API_KEY=\"XXXXXX\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Use Python-dotenv to load environment variables from <code>.env</code>.</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # take environment variables from .env.\n</code></pre>"},{"location":"quickstarts/agents/#create-a-basic-agent","title":"Create a Basic Agent","text":"<p>In <code>Floki</code>, tools bridge basic Python functions and <code>OpenAI's Function Calling</code> format, enabling seamless interaction between agents and external tasks. You can use <code>Pydantic</code> models to define the schema for tool arguments, ensuring structured input and validation.</p> <p>By annotating functions with <code>@tool</code> and specifying the argument schema, you transform them into <code>Agent tools</code> that can be invoke dynamically during workflows. This approach makes your tools compatible with LLM-driven decision-making and execution.</p> <pre><code>from floki import tool\nfrom pydantic import BaseModel, Field\n\nclass GetWeatherSchema(BaseModel):\n    location: str = Field(description=\"location to get weather for\")\n\n@tool(args_model=GetWeatherSchema)\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get weather information based on location.\"\"\"\n    import random\n    temperature = random.randint(60, 80)\n    return f\"{location}: {temperature}F.\"\n\nclass JumpSchema(BaseModel):\n    distance: str = Field(description=\"Distance for agent to jump\")\n\n@tool(args_model=JumpSchema)\ndef jump(distance: str) -&gt; str:\n    \"\"\"Jump a specific distance.\"\"\"\n    return f\"I jumped the following distance {distance}\"\n\ntools = [get_weather,jump]\n</code></pre> <p>Next, create your Agent by specifying key attributes such as <code>name</code>, <code>role</code>, <code>goal</code>, and <code>instructions</code>, while assigning the <code>tools</code> defined earlier. This setup equips your agent with a clear purpose and the ability to interact dynamically with its environment.</p> <pre><code>from floki import Agent\n\nAIAgent = Agent(\n    name = \"Stevie\",\n    role = \"Weather Assistant\",\n    goal = \"Assist Humans with weather related tasks.\",\n    instructions = [\"Get accurate weather information\", \"From time to time, you can Jump.\"],\n    tools=tools\n)\n</code></pre> <p>Finally, run the agent with a task.</p> <pre><code>AIAgent.run(\"What is the weather in Virgina, New York and Washington DC?\")\n</code></pre> <pre><code>user:\nWhat is the weather in Virgina, New York and Washington DC?\n------------------------------------------------------------------------------\nassistant(tool_call):\nFunction name: GetWeather (Call Id: call_QOxUTdkWXhA5hlaKfEmvY3As)\nArguments: {\"location\": \"Virginia\"}\n--------------------------------------------------------------------------------\nassistant(tool_call):\nFunction name: GetWeather (Call Id: call_brouCb5MnCgPbK172dKaw0cx)\nArguments: {\"location\": \"New York\"}\n--------------------------------------------------------------------------------\nassistant(tool_call):\nFunction name: GetWeather (Call Id: call_KUH1ErAHdMFV83gYYghRdaIK)\nArguments: {\"location\": \"Washington DC\"}\n-------------------------------------------------------------------------------\ntool(Id: call_QOxUTdkWXhA5hlaKfEmvY3As):\nVirginia: 68F.\n-------------------------------------------------------------------------------\ntool(Id: call_brouCb5MnCgPbK172dKaw0cx):\nNew York: 62F.\n--------------------------------------------------------------------------------\ntool(Id: call_KUH1ErAHdMFV83gYYghRdaIK):\nWashington DC: 67F.\n--------------------------------------------------------------------------------\nassistant:\nHere is the current weather for each location:\n- **Virginia**: 68\u00b0F\n- **New York**: 62\u00b0F\n- **Washington DC**: 67\u00b0F\n-------------------------------------------------------------------------------\n</code></pre> <p>You can check the agent's <code>chat_history</code> property.</p> <pre><code>AIAgent.chat_history\n</code></pre> <pre><code>[{'role': 'user',\n  'content': 'What is the weather in Virgina, New York and Washington DC?'},\n {'content': 'Here is the current weather for each location:\\n\\n- **Virginia**: 68\u00b0F\\n- **New York**: 62\u00b0F\\n- **Washington DC**: 67\u00b0F',\n  'role': 'assistant'}]\n</code></pre> <p>You can also reset the agent's memory:</p> <pre><code>AIAgent.reset_memory()\n</code></pre>"},{"location":"quickstarts/dapr_workflows/","title":"Dapr &amp; Floki Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>Dapr workflows provide a solid framework for managing long-running processes and interactions across distributed systems using the Dapr Python SDK. Floki builds on this by introducing tasks, which simplify defining and managing workflows while adding features like tool integrations and LLM-powered reasoning. This approach allows you to start with basic Dapr workflows and expand to more advanced capabilities, such as LLM-driven tasks or multi-agent coordination, as your needs grow.</p>"},{"location":"quickstarts/dapr_workflows/#default-dapr-workflows","title":"Default Dapr Workflows","text":"<p>Creating a Dapr workflow is straightforward. Start by creating a python script <code>wf_taskchain_original_activity.py</code> and initializing the <code>WorkflowRuntime</code>, which manages the execution of workflows.</p> <pre><code>import dapr.ext.workflow as wf\n\nwfr = wf.WorkflowRuntime()\n</code></pre> <p>Next, define the workflow logic and the individual tasks it will execute. In this example, we pass a number through a sequence of steps, where each step performs an operation on the input. This follows the Dapr Task chaining pattern commonly used in Dapr workflows.</p> <pre><code>@wfr.workflow(name='random_workflow')\ndef task_chain_workflow(ctx: wf.DaprWorkflowContext, wf_input: int):\n    result1 = yield ctx.call_activity(step1, input=wf_input)\n    result2 = yield ctx.call_activity(step2, input=result1)\n    result3 = yield ctx.call_activity(step3, input=result2)\n    return [result1, result2, result3]\n\n@wfr.activity\ndef step1(ctx, activity_input):\n    print(f'Step 1: Received input: {activity_input}.')\n    # Do some work\n    return activity_input + 1\n\n@wfr.activity\ndef step2(ctx, activity_input):\n    print(f'Step 2: Received input: {activity_input}.')\n    # Do some work\n    return activity_input * 2\n\n@wfr.activity\ndef step3(ctx, activity_input):\n    print(f'Step 3: Received input: {activity_input}.')\n    # Do some work\n    return activity_input ^ 2\n</code></pre> <p>Finally, start the <code>WorkflowRuntime</code>, create a <code>DaprWorkflowClient</code>, and schedule the workflow. The rest of the script monitors the workflow's progress and handles its completion.</p> <pre><code>from time import sleep\n\nif __name__ == '__main__':\n    wfr.start()\n    sleep(5)  # wait for workflow runtime to start\n\n    wf_client = wf.DaprWorkflowClient()\n    instance_id = wf_client.schedule_new_workflow(workflow=task_chain_workflow, input=10)\n    print(f'Workflow started. Instance ID: {instance_id}')\n    state = wf_client.wait_for_workflow_completion(instance_id)\n    print(f'Workflow completed! Status: {state.runtime_status}')\n\n    wfr.shutdown()\n</code></pre> <p>With this setup, you can easily implement and execute workflows using Dapr, leveraging its powerful runtime and task orchestration capabilities.</p>"},{"location":"quickstarts/dapr_workflows/#set-up-a-state-store-for-workflows","title":"Set Up a State Store for Workflows","text":"<p>Before running a workflow, you need to define a Dapr component for the state store. Workflows in Dapr leverage the actor model under the hood, which requires a state store to manage actor states and ensure reliability. When running locally, you can use the default Redis state store that was set up during <code>dapr init</code>. To configure the state store for workflows, we add an <code>actorStateStore</code> definition, ensuring workflows have the persistence they need for fault tolerance and long-running operations.</p> <p>Info</p> <p>Create a folder named <code>components</code> and create a <code>workflowstate.yaml</code> file inside with the following content. Name the state store <code>workflowstatestore</code>.</p> <pre><code>apiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: workflowstatestore\nspec:\n  type: state.redis\n  version: v1\n  initTimeout: 1m\n  metadata:\n  - name: redisHost\n    value: localhost:6379\n  - name: redisPassword\n    value: \"\"\n  - name: actorStateStore\n    value: \"true\"\n</code></pre>"},{"location":"quickstarts/dapr_workflows/#run-workflow","title":"Run Workflow!","text":"<p>Now, you can run that workflow with the <code>Dapr CLI</code>:</p> <pre><code>dapr run --app-id originalwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_original_activity.py\n</code></pre> <p></p>"},{"location":"quickstarts/dapr_workflows/#dapr-workflow-floki-workflows","title":"Dapr Workflow -&gt; Floki Workflows","text":"<p>With <code>Floki</code>, the goal was to simplify workflows while adding flexibility and powerful integrations. I wanted to create a way to track the workflow state, including input, output, and status, while also streamlining monitoring. To achieve this, I built additional <code>workflow</code> and <code>activity</code> wrappers. The workflow wrapper stays mostly the same as Dapr's original, but the activity wrapper has been extended into a <code>task wrapper</code>. This change allows tasks to integrate seamlessly with LLM-based prompts and other advanced capabilities.</p> <p>Info</p> <p>The same example as before can be written in the following way. While the difference might not be immediately noticeable, this is a straightforward example of task chaining using Python functions. Create a file named <code>wf_taskchain_floki_activity.py</code>.</p> <pre><code>from floki import WorkflowApp\nfrom floki.types import DaprWorkflowContext\n\nwfapp = WorkflowApp()\n\n@wfapp.workflow(name='random_workflow')\ndef task_chain_workflow(ctx:DaprWorkflowContext, input: int):\n    result1 = yield ctx.call_activity(step1, input=input)\n    result2 = yield ctx.call_activity(step2, input=result1)\n    result3 = yield ctx.call_activity(step3, input=result2)\n    return [result1, result2, result3]\n\n@wfapp.task\ndef step1(activity_input):\n    print(f'Step 1: Received input: {activity_input}.')\n    # Do some work\n    return activity_input + 1\n\n@wfapp.task\ndef step2(activity_input):\n    print(f'Step 2: Received input: {activity_input}.')\n    # Do some work\n    return activity_input * 2\n\n@wfapp.task\ndef step3(activity_input):\n    print(f'Step 3: Received input: {activity_input}.')\n    # Do some work\n    return activity_input ^ 2\n\nif __name__ == '__main__':\n    results = wfapp.run_and_monitor_workflow(task_chain_workflow, input=10)\n    print(f\"Results: {results}\")\n</code></pre> <p>Now, you can run that workflow with the same command with the <code>Dapr CLI</code>:</p> <pre><code>dapr run --app-id flokiwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_floki_activity.py\n</code></pre> <p></p> <p>If we inspect the <code>Workflow State</code> in the state store, you would see something like this:</p> <pre><code>{\n    \"instances\":{\n        \"a0d2de00818e4f0098318f0bed5fa1ee\":{\n            \"input\":\"10\",\n            \"output\":\"[11, 22, 20]\",\n            \"status\":\"completed\",\n            \"start_time\":\"2024-11-27T16:57:14.465235\",\n            \"end_time\":\"2024-11-27T16:57:17.540588\",\n            \"messages\":[]\n        }\n    }\n}\n</code></pre> <p><code>Floki</code> processes the workflow execution and even extracts the final output.</p>"},{"location":"quickstarts/llm/","title":"LLM Inference Client","text":"<p>In <code>Floki</code>, the LLM Inference Client is responsible for interacting with language models. It serves as the interface through which the agent communicates with the LLM, generating responses based on the input provided.</p> <p>Info</p> <p>By default, <code>Floki</code> uses the <code>OpenAIChatClient</code> to interact with the OpenAI Chat endpoint. By default, the <code>OpenAIChatClient</code> uses the <code>gpt-4o</code> model</p>"},{"location":"quickstarts/llm/#set-environment-variables","title":"Set Environment Variables","text":"<p>Create an <code>.env</code> file for your API keys and other environment variables with sensitive information that you do not want to hardcode.</p> <pre><code>OPENAI_API_KEY=\"XXXXXX\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Use Python-dotenv to load environment variables from <code>.env</code>.</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # take environment variables from .env.\n</code></pre>"},{"location":"quickstarts/llm/#basic-example","title":"Basic Example","text":"<p>By default, you can easily initialize the <code>OpenAIChatClient</code> without additional configuration. It uses the <code>OpenAI API</code> key from your environment variables.</p> <pre><code>from floki import OpenAIChatClient\n\nllm = OpenAIChatClient()\n\nllm.generate('Name a famous dog!')\n</code></pre> <p>This will generate a response using the <code>OpenAI</code> model, querying for the name of a famous dog.</p> <pre><code>ChatCompletion(choices=[Choice(finish_reason='stop', index=0, message=MessageContent(content='One famous dog is Lassie, the Rough Collie who became popular through films, television series, and books starting in the 1940s.', role='assistant'), logprobs=None)], created=1732713689, id='chatcmpl-AYCGvJxgP61OPy96Z3YW2dLAz7IJW', model='gpt-4o-2024-08-06', object='chat.completion', usage={'completion_tokens': 30, 'prompt_tokens': 12, 'total_tokens': 42, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}})\n</code></pre>"},{"location":"quickstarts/llm/#structured-output","title":"Structured Output","text":"<p>Onge again, initialize <code>OpenAIChatClient</code>.</p> <pre><code>from floki import OpenAIChatClient\n\nllmClient = OpenAIChatClient()\n</code></pre> <p>Define the response structure. You can use pydantic models.</p> <pre><code>from pydantic import BaseModel\n\nclass dog(BaseModel):\n    name: str\n    breed: str\n    reason: str\n</code></pre> <p>Finally, you can pass the response model to the LLM Client call.</p> <pre><code>from floki.types import UserMessage\n\nresponse = llmClient.generate(\n    messages=[UserMessage(\"One famous dog in history.\")],\n    response_model=dog\n)\nresponse\n</code></pre> <pre><code>dog(name='Hachiko', breed='Akita', reason=\"known for his remarkable loyalty to his owner, even many years after his owner's death\")\n</code></pre>"},{"location":"quickstarts/llm_workflows/","title":"LLM-based Task Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>In <code>Floki</code>, LLM-based Task Workflows allow developers to design step-by-step workflows where LLMs provide reasoning and decision-making at defined stages. These workflows are deterministic and structured, enabling the execution of tasks in a specific order, often defined by Python functions. This approach does not rely on event-driven systems or pub/sub messaging but focuses on defining and orchestrating tasks with the help of LLM reasoning when necessary. Ideal for scenarios that require a predefined flow of tasks enhanced by language model insights.</p> <p>Now that we have a better understanding of <code>Dapr</code> and <code>Floki</code> workflows, let\u2019s explore how to use Dapr activities or Floki tasks to call LLM Inference APIs, such as OpenAI Tex Generation endpoint, with models like <code>gpt-4o</code>.</p>"},{"location":"quickstarts/llm_workflows/#dapr-workflows-llm-inference-apis","title":"Dapr Workflows &amp; LLM Inference APIs","text":"<p>To start, we can define a few <code>Dapr</code> activities that interact with <code>OpenAI APIs</code>. These activities can be chained together so the output of one step becomes the input for the next. For example, in the first step, we can use the LLM\u2019s parameteric knowledge to pick a random character from The Lord of the Rings. In the second step, the LLM can generate a famous line spoken by that character.</p> <p>Tip</p> <p>Make sure you have your environment variables set up in an <code>.env</code> file so that the library can pick it up and use it to communicate with <code>OpenAI</code> services. We set them up in the LLM Inference Client section</p> <p>Start by initializing the <code>WorkflowRuntime</code> and gathering the right environment variables.</p> <pre><code>import dapr.ext.workflow as wf\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Initialize Workflow Instance\nwfr = wf.WorkflowRuntime()\n</code></pre> <p>Next, let's define a workflow and activities, demonstrating how each step integrates with the OpenAI Python SDK to achieve this functionality.</p> <pre><code># Define Workflow logic\n@wfr.workflow(name='lotr_workflow')\ndef task_chain_workflow(ctx: wf.DaprWorkflowContext):\n    result1 = yield ctx.call_activity(get_character)\n    result2 = yield ctx.call_activity(get_line, input=result1)\n    return result2\n\n# Activity 1\n@wfr.activity(name='step1')\ndef get_character(ctx):\n    client = OpenAI()\n    response = client.chat.completions.create(\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": \"Pick a random character from The Lord of the Rings and respond with the character name only\"\n            }\n        ],\n        model = 'gpt-4o'\n    )\n    character = response.choices[0].message.content\n    print(f\"Character: {character}\")\n    return character\n\n# Activity 2\n@wfr.activity(name='step2')\ndef get_line(ctx, character: str):\n    client = OpenAI()\n    response = client.chat.completions.create(\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"What is a famous line by {character}\"\n            }\n        ],\n        model = 'gpt-4o'\n    )\n    line = response.choices[0].message.content\n    print(f\"Line: {line}\")\n    return line\n</code></pre> <p>Finally, we complete the process by triggering the workflow and handling the output, including waiting for the workflow to finish before processing the results.</p> <pre><code>from time import sleep\n\nif __name__ == '__main__':\n    wfr.start()\n    sleep(5)  # wait for workflow runtime to start\n\n    wf_client = wf.DaprWorkflowClient()\n    instance_id = wf_client.schedule_new_workflow(workflow=task_chain_workflow)\n    print(f'Workflow started. Instance ID: {instance_id}')\n    state = wf_client.wait_for_workflow_completion(instance_id)\n    print(f'Workflow completed! Status: {state.runtime_status}')\n\n    wfr.shutdown()\n</code></pre> <p>Tip</p> <p>Before running a workflow, remember that you need to define a Dapr component for the state store.</p> <pre><code>dapr run --app-id originalllmwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_openai_original_llm_request.py\n</code></pre> <p></p>"},{"location":"quickstarts/llm_workflows/#floki-llm-based-tasks","title":"Floki LLM-based Tasks","text":"<p>Now, let\u2019s get to the exciting part! <code>Tasks</code> in <code>Floki</code> build on the concept of <code>activities</code> and bring additional flexibility. Using Python function signatures, you can define tasks with ease. The <code>task decorator</code> allows you to provide a <code>description</code> parameter, which acts as a prompt for the default LLM inference client in <code>Floki</code> (<code>OpenAIChatClient</code> by default).</p> <p>You can also use function arguments to pass variables to the prompt, letting you dynamically format the prompt before it\u2019s sent to the text generation endpoint. This makes it simple to implement workflows that follow the Dapr Task chaining pattern, just like in the earlier example, but with even more flexibility.</p> <pre><code>from floki import WorkflowApp\nfrom floki.types import DaprWorkflowContext\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Initialize the WorkflowApp\nwfapp = WorkflowApp()\n\n# Define Workflow logic\n@wfapp.workflow(name='lotr_workflow')\ndef task_chain_workflow(ctx: DaprWorkflowContext):\n    result1 = yield ctx.call_activity(get_character)\n    result2 = yield ctx.call_activity(get_line, input={\"character\": result1})\n    return result2\n\n@wfapp.task(description=\"\"\"\n    Pick a random character from The Lord of the Rings\\n\n    and respond with the character's name only\n\"\"\")\ndef get_character() -&gt; str:\n    pass\n\n@wfapp.task(description=\"What is a famous line by {character}\",)\ndef get_line(character: str) -&gt; str:\n    pass\n\nif __name__ == '__main__':\n    results = wfapp.run_and_monitor_workflow(task_chain_workflow)\n    print(f\"Famous Line: {results}\")\n</code></pre> <p>Run the workflow with the following command:</p> <pre><code>dapr run --app-id flokillmmwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_openai_floki_llm_request.py\n</code></pre> <p></p>"}]}