{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Floki: Agentic Workflows Made Simple","text":"<p>Floki is an open-source framework for researchers and developers to experiment with LLM-based autonomous agents. It provides tools to create, orchestrate, and manage agents while seamlessly connecting to LLM inference APIs. Built on Dapr, Floki leverages a unified programming model that simplifies microservices and supports both deterministic workflows and event-driven interactions. Using Dapr\u2019s Virtual Actor pattern, Floki enables agents to function as independent, self-contained units that process messages sequentially, eliminating concurrency concerns while seamlessly integrating into larger workflows. It also facilitates agent collaboration through Dapr\u2019s Pub/Sub integration, where agents communicate via a shared message bus, simplifying the design of workflows where tasks are distributed efficiently, and agents work together to achieve shared goals. By bringing together these features, Floki provides a powerful way to explore agentic workflows and the components that enable multi-agent systems to collaborate and scale, all powered by Dapr.</p>"},{"location":"#why-dapr","title":"Why Dapr \ud83c\udfa9?","text":"<p>Dapr provides Floki with a unified programming model that simplifies the development of resilient and scalable systems by offering built-in APIs for features such as service invocation, Pub/Sub messaging, workflows, and even state management. These components, essential for defining agentic workflows, allow developers to focus on designing agents and workflows rather than rebuilding foundational features. By leveraging Dapr\u2019s sidecar architecture and portable, event-driven runtime, Floki also enables agents to collaborate effectively, share tasks, and adapt dynamically across cloud and edge environments. This seamless integration brings together deterministic workflows and LLM-based decision-making into a unified system, making it easier to experiment with multi-agent systems and scalable agentic workflows.</p>"},{"location":"#key-dapr-features-in-floki","title":"Key Dapr Features in Floki:","text":"<ul> <li>\ud83c\udfaf Service-to-Service Invocation: Facilitates direct communication between agents with built-in service discovery, error handling, and distributed tracing. Agents can leverage this for synchronous messaging in multi-agent workflows.</li> <li>\u26a1\ufe0f Publish and Subscribe: Supports loosely coupled collaboration between agents through a shared message bus. This enables real-time, event-driven interactions critical for task distribution and coordination.</li> <li>\ud83d\udd04 Workflow API: Defines long-running, persistent workflows that combine deterministic processes with LLM-based decision-making. Floki uses this to orchestrate complex multi-step agentic workflows seamlessly.</li> <li>\ud83e\udde0 State Management: Provides a flexible key-value store for agents to retain context across interactions, ensuring continuity and adaptability during workflows.</li> <li>\ud83e\udd16 Actors: Implements the Virtual Actor pattern, allowing agents to operate as self-contained, stateful units that handle messages sequentially. This eliminates concurrency concerns and enhances scalability in Floki's agent systems.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Set up in 2 minutes</p> <p>Install <code>Floki</code> with <code>pip</code> and set up your dapr environment in minutes</p> <p> Installation</p> </li> <li> <p> Start experimenting</p> <p>Build your first agent and design a custom workflow to get started with Floki.</p> <p> Quickstarts</p> </li> <li> <p> Learn more</p> <p>Learn more about Floki and its main components!</p> <p> Concepts</p> </li> <li> <p> Open Source, MIT</p> <p>Floki is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"home/installation/","title":"Installation","text":""},{"location":"home/installation/#install-floki","title":"Install Floki","text":"<p>Info</p> <p>make sure you have Python already installed. <code>Python &gt;=3.9</code></p>"},{"location":"home/installation/#as-a-python-package-using-pip","title":"As a Python package using Pip","text":"<pre><code>pip install floki-ai\n</code></pre>"},{"location":"home/installation/#remotely-from-github","title":"Remotely from GitHub","text":"<pre><code>pip install git+https://github.com/Cyb3rWard0g/floki.git\n</code></pre>"},{"location":"home/installation/#from-source-with-poetry","title":"From source with <code>poetry</code>:","text":"<pre><code>git clone https://github.com/Cyb3rWard0g/floki\n\ncd floki\n\npoetry install\n</code></pre>"},{"location":"home/installation/#install-dapr-cli","title":"Install Dapr CLI","text":"<p>Install the Dapr CLI to manage Dapr-related tasks like running applications with sidecars, viewing logs, and launching the Dapr dashboard. It works seamlessly with both self-hosted and Kubernetes environments. For a complete step-by-step guide, visit the official Dapr CLI installation page.</p> <p>Verify the CLI is installed by restarting your terminal/command prompt and running the following:</p> <pre><code>dapr -h\n</code></pre>"},{"location":"home/installation/#initialize-dapr-in-local-mode","title":"Initialize Dapr in Local Mode","text":"<p>Info</p> <p>Make sure you have Docker already installed. I use Docker Desktop.</p> <p>Initialize Dapr locally to set up a self-hosted environment for development. This process installs Dapr sidecar binaries, runs essential services like Redis (state store and message broker) and Zipkin (observability), and prepares a default components folder. For detailed steps, see the official guide on initializing Dapr locally.</p> <p>To initialize the Dapr control plane containers and create a default configuration file, run:</p> <pre><code>dapr init\n</code></pre> <p>Verify you have container instances with <code>daprio/dapr</code>, <code>openzipkin/zipkin</code>, and <code>redis</code> images running:</p> <pre><code>docker ps\n</code></pre>"},{"location":"home/concepts/agents/","title":"Agents","text":"<p>Agents in <code>Floki</code> are autonomous systems powered by Large Language Models (LLMs), designed to execute tasks, reason through problems, and collaborate within workflows. Acting as intelligent building blocks, agents seamlessly combine LLM-driven reasoning with tool integration, memory, and collaboration features to enable scalable, agentic systems.</p> <p></p>"},{"location":"home/concepts/agents/#core-features","title":"Core Features","text":""},{"location":"home/concepts/agents/#1-llm-integration","title":"1. LLM Integration","text":"<p>Floki provides a unified interface to connect with LLM inference APIs, starting with OpenAI models. This abstraction allows developers to seamlessly integrate their agents with cutting-edge language models for reasoning and decision-making.</p>"},{"location":"home/concepts/agents/#2-structured-outputs","title":"2. Structured Outputs","text":"<p>Agents in Floki leverage structured output capabilities, such as OpenAI\u2019s Function Calling, to generate predictable and reliable results. These outputs follow JSON Schema Draft 2020-12 and OpenAPI Specification v3.1.0 standards, enabling easy interoperability and tool integration.</p>"},{"location":"home/concepts/agents/#3-tool-selection","title":"3. Tool Selection","text":"<p>Agents dynamically select the appropriate tool for a given task, using LLMs to analyze requirements and choose the best action. This is supported directly through LLM parametric knowledge and enhanced by Function Calling, ensuring tools are invoked efficiently and accurately.</p>"},{"location":"home/concepts/agents/#4-memory","title":"4. Memory","text":"<p>Agents retain context across interactions, enhancing their ability to provide coherent and adaptive responses. Memory options range from simple in-memory lists for managing chat history to vector databases for semantic search and retrieval. Floki also integrates with Dapr state stores, enabling scalable and persistent memory for advanced use cases.</p>"},{"location":"home/concepts/agents/#5-prompt-flexibility","title":"5. Prompt Flexibility","text":"<p>Floki supports flexible prompt templates to shape agent behavior and reasoning. Users can define placeholders within prompts, enabling dynamic input of context for inference calls. By leveraging prompt formatting with Jinja templates, users can include loops, conditions, and variables, providing precise control over the structure and content of prompts. This flexibility ensures that LLM responses are tailored to the task at hand, offering modularity and adaptability for diverse use cases.</p>"},{"location":"home/concepts/agents/#6-agent-services","title":"6. Agent Services","text":"<p>Agents are exposed as independent services using FastAPI and Dapr applications. This modular approach separates the agent\u2019s logic from its service layer, enabling seamless reuse, deployment, and integration into multi-agent systems.</p>"},{"location":"home/concepts/agents/#7-message-driven-communication","title":"7. Message-Driven Communication","text":"<p>Agents collaborate through Pub/Sub messaging, enabling event-driven communication and task distribution. This message-driven architecture allows agents to work asynchronously, share updates, and respond to real-time events, ensuring effective collaboration in distributed systems.</p>"},{"location":"home/concepts/agents/#8-workflow-orchestration","title":"8. Workflow Orchestration","text":"<p>Floki supports both deterministic and event-driven workflows to manage multi-agent systems via Dapr Workflows. Deterministic workflows provide clear, repeatable processes, while event-driven workflows allow for dynamic, adaptive collaboration between agents in centralized or decentralized architectures.</p>"},{"location":"home/concepts/agents/#agent-patterns","title":"Agent Patterns","text":"<p>In Floki, Agent Patterns define the built-in loops that allow agents to dynamically handle tasks. These patterns enable agents to iteratively reason, act, and adapt, making them flexible and capable problem-solvers. By embedding these patterns, Floki ensures agents can independently complete tasks without requiring external orchestration.</p>"},{"location":"home/concepts/agents/#tool-calling","title":"Tool Calling","text":"<p>Tool Calling is an essential pattern in autonomous agent design, allowing AI agents to interact dynamically with external tools based on user input. One reliable method for enabling this is through OpenAI's Function Calling capabilities, introduced on June 13, 2023. This feature allows developers to describe functions to models trained to generate structured JSON objects containing the necessary arguments for tool execution, based on user queries.</p>"},{"location":"home/concepts/agents/#how-it-works","title":"How It Works","text":"<ol> <li>The user submits a query specifying a task and the available tools.</li> <li>The LLM analyzes the query and selects the right tool for the task.</li> <li>The LLM provides a structured JSON output containing the tool\u2019s unique ID, name, and arguments.</li> <li>The AI agent parses the JSON, executes the tool with the provided arguments, and sends the results back as a tool message.</li> <li>The LLM then summarizes the tool's execution results within the user\u2019s context to deliver a comprehensive final response.</li> </ol> <p>Info</p> <p>Steps 2-4 can be repeated multiple times, depending on the task's complexity.</p> <p>This pattern is highly flexible and supports multiple iterations of tool selection and execution, empowering agents to handle dynamic and multi-step tasks more effectively.</p>"},{"location":"home/concepts/agents/#react","title":"ReAct","text":"<p>The ReAct (Reason + Act) pattern was introduced in 2022 to enhance the capabilities of LLM-based AI agents by combining reasoning with action. This approach allows agents not only to reason through complex tasks but also to interact with the environment, taking actions based on their reasoning and observing the outcomes. ReAct enables AI agents to dynamically adapt to tasks by reasoning about the next steps and executing actions in real time.</p>"},{"location":"home/concepts/agents/#how-it-works_1","title":"How It Works","text":"<ul> <li>Thought (Reasoning): The agent analyzes the situation and generates a thought or a plan based on the input.</li> <li>Action: The agent takes an action based on its reasoning.</li> <li>Observation: After the action is executed, the agent observes the results or feedback from the environment, assessing the effectiveness of its action.</li> </ul> <p>Info</p> <p>These steps create a cycle that allows the agent to continuously think, act, and learn from the results.</p> <p>The ReAct pattern gives the agent the flexibility to adapt based on task complexity:</p> <ul> <li>Deep reasoning tasks: The agent goes through multiple cycles of thought, action, and observation to refine its responses.</li> <li>Action-driven tasks: The agent focuses more on timely actions and thinks critically only at key decision points.</li> </ul> <p>ReAct empowers agents to navigate complex, real-world environments efficiently, making them better suited for scenarios that require both deep reasoning and timely actions.</p>"},{"location":"home/concepts/agents/#workflows-for-collaboration","title":"Workflows for Collaboration","text":"<p>While patterns empower individual agents, workflows enable the coordination of multiple agents to achieve shared goals. In Floki, workflows serve as a higher-level framework for organizing how agents collaborate and distribute tasks.</p> <p>Workflows can orchestrate agents, each equipped with their own built-in patterns, to handle different parts of a larger process. For example, one agent might gather data using tools, another might analyze the results, and a third might generate a report. The workflow manages the communication and sequencing between these agents, ensuring smooth collaboration.</p> <p>Interestingly, workflows can also define loops similar to agent patterns. Instead of relying on an agent\u2019s built-in tool-calling loop, you can design workflows to orchestrate tool usage, reasoning, and action. This gives you the flexibility to use workflows to define both multi-agent collaboration and complex task handling for a single agent.</p>"},{"location":"home/concepts/agents/#random-workflow","title":"Random Workflow","text":"<p>In a Random Workflow, the next agent to handle a task is selected randomly. This approach:</p> <ul> <li>Encourages diversity in agent responses and strategies.</li> <li>Simplifies orchestration in cases where task assignment does not depend on specific agent roles or expertise.</li> <li>Random workflows are particularly useful for exploratory tasks or brainstorming scenarios where agent collaboration benefits from randomness.</li> </ul>"},{"location":"home/concepts/agents/#round-robin-workflow","title":"Round Robin Workflow","text":"<p>The Round Robin Workflow assigns tasks sequentially to agents in a fixed order. This method:</p> <ul> <li>Ensures equal participation among agents.</li> <li>Is ideal for scenarios requiring predictable task distribution, such as routine monitoring or repetitive processes.</li> <li>For example, in a team of monitoring agents, each agent takes turns analyzing incoming data streams in a predefined order.</li> </ul>"},{"location":"home/concepts/agents/#llm-based-workflow","title":"LLM-Based Workflow","text":"<p>The LLM-Based Workflow relies on the reasoning capabilities of an LLM to dynamically choose the next agent based on:</p> <ul> <li>Task Context: The nature and requirements of the current task.</li> <li>Chat History: Previous agent responses and interactions.</li> <li>Agent Metadata: Attributes like expertise, availability, or priorities.</li> </ul> <p>This approach ensures that the most suitable agent is selected for each task, optimizing collaboration and efficiency. For example, in a multi-agent customer support system, the LLM can assign tasks to agents based on customer issues, agent expertise, and workload distribution.</p>"},{"location":"home/concepts/floki/","title":"Enter Floki!","text":"<p>Floki is an open-source framework for building and orchestrating LLM-based autonomous agents, designed to simplify the complexity of creating scalable agentic workflows and microservices. Inspired by the growing need for frameworks that integrate seamlessly with distributed systems, Floki enables developers to focus on designing intelligent agents without getting bogged down by infrastructure concerns.</p>"},{"location":"home/concepts/floki/#the-problem","title":"The Problem","text":"<p>Many agentic frameworks today attempt to redefine how microservices are built and orchestrated by developing their own platforms for workflows, Pub/Sub messaging, state management, and service communication. While these efforts showcase innovation, they often lead to a steep learning curve, fragmented systems, and unnecessary complexity when scaling or adapting to new environments.</p> <p>Rather than building on existing solutions that are proven to handle these challenges at scale, many frameworks require developers to adopt entirely new paradigms or recreate foundational infrastructure. This added complexity often diverts focus from the primary goal: designing and implementing intelligent, effective agents.</p>"},{"location":"home/concepts/floki/#flokis-approach","title":"Floki's Approach","text":"<p>Floki takes a distinct approach by building on Dapr, a portable and event-driven runtime optimized for distributed systems. Dapr offers built-in APIs and patterns such as state management, Pub/Sub messaging, service invocation, and virtual actors\u2014that eliminate the need to recreate foundational components from scratch. By integrating seamlessly with Dapr, Floki empowers developers to focus on the intelligence and behavior of LLM-powered agents while leveraging a proven framework for scalability and reliability.</p> <p>Rather than reinventing microservices, Floki enables developers to design, test, and deploy agents that seamlessly integrate as collaborative services within larger systems. Whether experimenting with a single agent or orchestrating workflows involving multiple agents, Floki simplifies the exploration and implementation of scalable agentic workflows.</p>"},{"location":"home/concepts/floki/#conclusion","title":"Conclusion","text":"<p>Floki provides a unified framework for designing, deploying, and orchestrating LLM-powered agents. By leveraging Dapr\u2019s runtime and modular components, Floki allows developers to focus on building intelligent systems without worrying about the complexities of distributed infrastructure. Whether you're creating standalone agents or orchestrating multi-agent workflows, Floki empowers you to explore the future of intelligent, scalable, and collaborative systems.</p>"},{"location":"home/concepts/floki/#why-the-name-floki","title":"Why the Name Floki?","text":"<p>The name <code>Floki</code> is inspired by both history and fiction. Historically, Floki Vilger\u00f0arson is known in Norse sagas as the first Norseman to journey to Iceland, embodying a spirit of discovery. In the Vikings series, Floki is portrayed as a skilled boat builder, creating vessels that allowed his people to explore and achieve their goals.</p> <p>In the same way, this framework equips developers with the tools to build, prototype, and deploy their own agents or fleets of agents, enabling them to experiment and explore the potential of LLM-based workflows.</p>"},{"location":"home/concepts/principles/","title":"Core Principles","text":""},{"location":"home/concepts/principles/#1-powered-by-dapr","title":"1. Powered by Dapr","text":"<p>Floki leverages Dapr\u2019s modular and scalable architecture to provide a foundation that simplifies:</p> <ul> <li>Service communication: Built-in APIs for service-to-service invocation reduce the complexity of connecting agents and services.</li> <li>Cloud integration: Deploy seamlessly across local, cloud, or hybrid environments.</li> <li>Event-driven collaboration: Agents communicate effectively through Pub/Sub messaging, enabling dynamic task distribution and real-time updates.</li> </ul>"},{"location":"home/concepts/principles/#2-modular-component-architecture","title":"2. Modular Component Architecture","text":"<p>Floki inherits Dapr\u2019s pluggable architecture, allowing developers to:</p> <ul> <li>Swap components like Pub/Sub systems (e.g., RabbitMQ \u2192 Kafka) or state stores (e.g., Redis \u2192 DynamoDB) without modifying application code.</li> <li>Experiment with different setups locally or in the cloud, ensuring flexibility and adaptability.</li> <li>Scale effortlessly by relying on modular, production-ready building blocks.</li> </ul>"},{"location":"home/concepts/principles/#3-virtual-actor-model-for-agents","title":"3. Virtual Actor Model for Agents","text":"<p>Floki uses Dapr\u2019s Virtual Actor model to enable agents to act as:</p> <ul> <li>Stateful entities: Agents can retain context across interactions, ensuring continuity in workflows.</li> <li>Independent units: Each agent processes tasks sequentially, simplifying concurrency management.</li> <li>Scalable services: Agents scale seamlessly across nodes in a cluster, adapting to distributed environments.</li> </ul>"},{"location":"home/concepts/principles/#4-message-driven-communication","title":"4. Message-Driven Communication","text":"<p>Floki emphasizes the use of Pub/Sub messaging to empower both event-driven and deterministic workflows. This principle ensures:</p> <ul> <li>Decoupled Architecture: Agents and services can communicate asynchronously, promoting scalability and flexibility.</li> <li>Event-Driven Interactions: Agents respond to real-time events, allowing workflows to adapt dynamically.</li> <li>Workflow Enablement: Pub/Sub acts as a backbone for coordinating agents in centralized or decentralized workflows.</li> </ul>"},{"location":"home/concepts/principles/#5-workflow-oriented-design","title":"5. Workflow-Oriented Design","text":"<p>Floki integrates Dapr\u2019s Workflow API to orchestrate:</p> <ul> <li>Deterministic workflows: Define repeatable processes for agents to follow.</li> <li>Dynamic decision-making: Combine rule-based logic with LLM-driven reasoning to handle complex, adaptive tasks.</li> <li>Multi-agent collaboration: Coordinate agents across workflows for shared goals and efficient task execution.</li> </ul>"},{"location":"home/quickstarts/","title":"Floki Quickstarts","text":"<p>Dive into our Floki quickstarts to explore core features with practical code samples, designed to get you up and running quickly. From setup to hands-on examples, these resources are your first step into the world of Floki.</p> <p>Info</p> <p>Not all quickstarts require Docker, but it is recommended to have your local Dapr environment set up with Docker for the best development experience and to follow the steps in this guide seamlessly.</p>"},{"location":"home/quickstarts/#quickstarts","title":"Quickstarts","text":"Scenario Description LLM Inference Client Learn how to set up and use Floki's LLM Inference Client to interact with language models like OpenAI's <code>gpt-4o</code>. This quickstart covers initializing the OpenAIChatClient, managing environment variables, and generating structured responses using Pydantic models. LLM-based AI Agents Discover how to create LLM-based autonomous agents. This quickstart walks you through defining tools with Pydantic schemas, setting up agents with clear roles and goals, and enabling dynamic task execution using OpenAI's Function Calling. Dapr &amp; Floki Workflows Explore how Floki builds on Dapr workflows to simplify long-running process management. Learn how to define tasks, integrate tools, and add LLM reasoning to extend workflow capabilities. LLM-based Task Workflows Design structured, step-by-step workflows with LLMs providing reasoning at key stages. This quickstart covers task orchestration with Python functions and integrating LLM Inference APIs. Event-Driven Agentic Workflows Leverage event-driven systems with pub/sub messaging to enable agents to collaborate dynamically. This quickstart demonstrates setting up workflows for decentralized, real-time agent interaction."},{"location":"home/quickstarts/agentic_workflows/","title":"Event-Driven Agentic Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>Event-Driven Agentic Workflows in <code>Floki</code> take advantage of an event-driven system using pub/sub messaging and a shared message bus. Agents operate as autonomous entities that respond to events dynamically, enabling real-time interactions and collaboration. These workflows are highly adaptable, allowing agents to communicate, share tasks, and reason through events triggered by their environment. This approach is best suited for decentralized systems requiring dynamic agent collaboration across distributed applications.</p>"},{"location":"home/quickstarts/agentic_workflows/#agents-as-services","title":"Agents as Services","text":"<p>In <code>Floki</code>, agents can be exposed as services, making them reusable, modular, and easy to integrate into event-driven workflows. Each agent runs as a microservice, wrapped in a Dapr-enabled FastAPI server. This design allows agents to operate independently while communicating through Dapr\u2019s pub/sub messaging and interacting with state stores or other services.</p> <p>The way to structure such a project is straightforward. We organize our services into a directory that contains individual folders for each agent, along with a components/ directory for Dapr configurations. Each agent service includes its own app.py file, where the FastAPI server and the agent logic are defined.</p> <pre><code>components/                # Dapr configuration files\n\u251c\u2500\u2500 statestore.yaml        # State store configuration\n\u251c\u2500\u2500 pubsub.yaml            # Pub/Sub configuration\n\u2514\u2500\u2500 ...                    # Other Dapr components\nservices/                  # Directory for agent services\n\u251c\u2500\u2500 agent1/                # First agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent1\n\u2502   \u2514\u2500\u2500 ...                # Additional agent1 files\n\u2502\u2500\u2500 agent2/                # Second agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent2\n\u2502   \u2514\u2500\u2500 ...                # Additional agent2 files\n\u2514\u2500\u2500 ...                    # More agents\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#your-first-service","title":"Your First Service","text":"<p>Let's start by definining a <code>Hobbit</code> service with a specific <code>name</code>, <code>role</code>, <code>goal</code> and <code>instructions</code>.</p> <pre><code>services/                  # Directory for agent services\n\u251c\u2500\u2500 hobbit/                # Hobbit Service\n\u2502   \u251c\u2500\u2500 app.py             # Dapr Enabled FastAPI app for Hobbit\n</code></pre> <p>Create the <code>app.py</code> script and provide the following information.</p> <pre><code>from floki import Agent, AgentService\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        # Define Agent\n        hobbit_agent = Agent(\n            role=\"Hobbit\",\n            name=\"Frodo\",\n            goal=\"Take the ring to Mordor\",\n            instructions=[\"Speak like Frodo\"]\n        )\n        # Expose Agent as a Service\n        hobbit_service = AgentService(\n            agent=hobbit_agent,\n            message_bus_name=\"messagepubsub\",\n            agents_state_store_name=\"agentstatestore\",\n            port=8001,\n            daprGrpcPort=50001\n        )\n        await hobbit_service.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>Now, you can define multiple services following this format, but it's essential to pay attention to key areas to ensure everything runs smoothly. Specifically, focus on correctly configuring the components (e.g., <code>statestore</code> and <code>pubsub</code> names) and incrementing the ports for each service.</p> <p>Key Considerations:</p> <ul> <li>Ensure the <code>message_bus_name</code> matches the <code>pub/sub</code> component name in your <code>pubsub.yaml</code> file.</li> <li>Verify the <code>agents_state_store_name</code> matches the state store component defined in your <code>statestore.yaml</code> file.</li> <li>Increment the port for each new agent service (e.g., 8001, 8002, 8003).</li> <li>Similarly, increment the <code>daprGrpcPort</code> for each service (e.g., 50001, 50002, 50003) to avoid conflicts.</li> <li>Customize the Agent parameters (<code>role</code>, <code>name</code>, <code>goal</code>, and <code>instructions</code>) to match the behavior you want for each service.</li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#the-agentic-workflow-service","title":"The Agentic Workflow Service","text":"<p>The Agentic Workflow Service in Floki extends workflows to orchestrate communication among agents. It allows you to send messages to agents to trigger their participation and monitors a shared message bus to listen for all messages being passed. This enables dynamic collaboration and task distribution among agents.</p> <p>Types of Agentic Workflows:</p> <ul> <li>Random: Distributes tasks to agents randomly, ensuring a non-deterministic selection of participating agents for each task.</li> <li>RoundRobin: Cycles through agents in a fixed order, ensuring each agent has an equal opportunity to participate in tasks.</li> <li>LLM-based: Leverages an LLM to decide which agent to trigger based on the content and context of the task and chat history.</li> </ul> <p>Next, we\u2019ll define a <code>Random Agentic Workflow Service</code> to demonstrate how this concept can be implemented.</p> <pre><code>from floki import RandomWorkflowService\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        random_workflow_service = RandomWorkflowService(\n            name=\"Orchestrator\",\n            message_bus_name=\"messagepubsub\",\n            agents_state_store_name=\"agentstatestore\",\n            workflow_state_store_name=\"workflowstatestore\",\n            port=8004,\n            daprGrpcPort=50004,\n            max_iterations=2\n        )\n\n        await random_workflow_service.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>Unlike <code>Agents as Services</code>, the <code>Agentic Workflow Service</code> does not require an agent parameter since it orchestrates communication among multiple agents rather than representing a single agent. Instead, the configuration focuses on workflow-specific parameters:</p> <ul> <li>Max Iterations: Defines the maximum number of iterations the workflow will perform, ensuring controlled task execution and preventing infinite loops.</li> <li>Workflow State Store Name: Specifies the state store used to persist the workflow\u2019s state, allowing for reliable recovery and tracking of workflow progress.</li> </ul> <p>These differences reflect the distinct purpose of the Agentic Workflow Service, which acts as a centralized orchestrator rather than an individual agent service.</p>"},{"location":"home/quickstarts/agentic_workflows/#the-multi-app-run-template-file","title":"The Multi-App Run template file","text":"<p>The Multi-App Run Template File is a YAML configuration file named <code>dapr.yaml</code> that allows you to run multiple applications simultaneously. This file is placed at the same level as the <code>components/</code> and <code>services/</code> directories, ensuring a consistent and organized project structure.</p> <pre><code>dapr.yaml                  # The Multi-App Run template\ncomponents/                # Dapr configuration files\n\u251c\u2500\u2500 statestore.yaml        # State store configuration\n\u251c\u2500\u2500 pubsub.yaml            # Pub/Sub configuration\n\u2514\u2500\u2500 ...                    # Other Dapr components\nservices/                  # Directory for agent services\n\u251c\u2500\u2500 agent1/                # First agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent1\n\u2502   \u2514\u2500\u2500 ...                # Additional agent1 files\n\u2502\u2500\u2500 agent2/                # Second agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent2\n\u2502   \u2514\u2500\u2500 ...                # Additional agent2 files\n\u2514\u2500\u2500 ...                    # More agents\n</code></pre> <p>Following our current scenario, we can set the following <code>Multi-App Run</code> template file:</p> <pre><code># https://docs.dapr.io/developing-applications/local-development/multi-app-dapr-run/multi-app-template/#template-properties\nversion: 1\ncommon:\n  resourcesPath: ./components\n  logLevel: info\n  appLogDestination: console\n  daprdLogDestination: console\n\napps:\n- appId: HobbitApp\n  appDirPath: ./services/hobbit/\n  appPort: 8001\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50001\n\n- appId: WizardApp\n  appDirPath: ./services/wizard/\n  appPort: 8002\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50002\n\n- appId: ElfApp\n  appDirPath: ./services/elf/\n  appPort: 8003\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50003\n\n- appId: WorkflowApp\n  appDirPath: ./services/workflow-random/\n  appPort: 8004\n  command: [\"python3\", \"app.py\"]\n  daprGRPCPort: 50004\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#starting-all-service-servers","title":"Starting All Service Servers","text":"<p>Tip</p> <p>Make sure you have your environment variables set up in an <code>.env</code> file so that the library can pick it up and use it to communicate with <code>OpenAI</code> services. We set them up in the LLM Inference Client section</p> <p>To start all the service servers defined in your project, you can use the Dapr CLI with the Multi-App Run template file. When you provide a directory path, the CLI will look for the dapr.yaml file (the default name for the template) in that directory. If the file is not found, the CLI will return an error.</p> <p>To execute the command, ensure you are in the root directory where the dapr.yaml file is located, then run:</p> <pre><code>dapr run -f .\n</code></pre> <p>This command reads the dapr.yaml file and starts all the services specified in the template.</p>"},{"location":"home/quickstarts/agentic_workflows/#start-workflow-via-an-http-request","title":"Start Workflow via an HTTP Request","text":"<p>Once all services are running, you can initiate the workflow by making an HTTP POST request to the Agentic Workflow Service. This service orchestrates the workflow, triggering agent actions and handling communication among agents.</p> <p>Here\u2019s an example of how to start the workflow using <code>curl</code>:</p> <pre><code>curl -i -X POST http://localhost:8004/RunWorkflow \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"message\": \"How to get to Mordor? Let's all help!\"}'\n</code></pre> <p>In this example:</p> <ul> <li>The request is sent to the Agentic Workflow Service running on port 8004.</li> <li>The message parameter is passed as input to the workflow, which the agents will process.</li> <li>This command demonstrates how to interact with the Agentic Workflow Service to kick off a new workflow.</li> </ul> <p>If you check the console where you started all the service servers, you will see the following output once the system reaches its maximum number of iterations.</p> <p></p>"},{"location":"home/quickstarts/agentic_workflows/#monitor-workflow-execution","title":"Monitor Workflow Execution","text":"<p>As mentioned earlier, when we ran dapr init, Dapr initialized, a <code>Zipkin</code> container instance, used for observability and tracing. We can use the Zipkin container to monitor our workflow execution. To do this, open your browser and go to <code>http://localhost:9411/zipkin/</code>. From there:</p> <p>Click on <code>Find a Trace</code> and then <code>Run Query</code> to search for traces.</p> <p></p> <p>Select the trace entry with multiple spans labeled <code>&lt;workflow name&gt;: /taskhubsidecarservice/startinstance.</code></p> <p>When you open this entry, you\u2019ll see details about how each task or activity in the workflow was executed. If any task failed, the error will also be visible here.</p> <p></p>"},{"location":"home/quickstarts/agents/","title":"LLM-based AI Agents","text":"<p>In the <code>Floki</code> framework, agents are autonomous systems powered by large language models (LLMs) that serve as their reasoning engine. These agents use the LLM\u2019s parametric knowledge to process information, reason in natural language, and interact dynamically with their environment by leveraging tools. Tools allow the agents to perform real-world tasks, gather new information, and adapt their reasoning based on feedback.</p> <p>Info</p> <p>By default, <code>Floki</code> sets the agentic pattern for the <code>Agent</code> class to <code>toolcalling</code> mode, enabling AI agents to interact dynamically with external tools using OpenAI's Function Calling.</p> <p><code>Tool Calling</code> empowers agents to identify the right tools for a task, format the necessary arguments, and execute the tools independently. The results are then passed back to the LLM for further processing, enabling seamless and adaptive agent workflows.</p>"},{"location":"home/quickstarts/agents/#environment-variables","title":"Environment Variables","text":"<p>Create an <code>.env</code> file for your API keys and other environment variables with sensitive information that you do not want to hardcode.</p> <pre><code>OPENAI_API_KEY=\"XXXXXX\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Use Python-dotenv to load environment variables from <code>.env</code>.</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # take environment variables from .env.\n</code></pre>"},{"location":"home/quickstarts/agents/#create-a-basic-agent","title":"Create a Basic Agent","text":"<p>In <code>Floki</code>, tools bridge basic Python functions and <code>OpenAI's Function Calling</code> format, enabling seamless interaction between agents and external tasks. You can use <code>Pydantic</code> models to define the schema for tool arguments, ensuring structured input and validation.</p> <p>By annotating functions with <code>@tool</code> and specifying the argument schema, you transform them into <code>Agent tools</code> that can be invoke dynamically during workflows. This approach makes your tools compatible with LLM-driven decision-making and execution.</p> <pre><code>from floki import tool\nfrom pydantic import BaseModel, Field\n\nclass GetWeatherSchema(BaseModel):\n    location: str = Field(description=\"location to get weather for\")\n\n@tool(args_model=GetWeatherSchema)\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get weather information based on location.\"\"\"\n    import random\n    temperature = random.randint(60, 80)\n    return f\"{location}: {temperature}F.\"\n\nclass JumpSchema(BaseModel):\n    distance: str = Field(description=\"Distance for agent to jump\")\n\n@tool(args_model=JumpSchema)\ndef jump(distance: str) -&gt; str:\n    \"\"\"Jump a specific distance.\"\"\"\n    return f\"I jumped the following distance {distance}\"\n\ntools = [get_weather,jump]\n</code></pre> <p>Next, create your Agent by specifying key attributes such as <code>name</code>, <code>role</code>, <code>goal</code>, and <code>instructions</code>, while assigning the <code>tools</code> defined earlier. This setup equips your agent with a clear purpose and the ability to interact dynamically with its environment.</p> <pre><code>from floki import Agent\n\nAIAgent = Agent(\n    name = \"Stevie\",\n    role = \"Weather Assistant\",\n    goal = \"Assist Humans with weather related tasks.\",\n    instructions = [\"Get accurate weather information\", \"From time to time, you can Jump.\"],\n    tools=tools\n)\n</code></pre> <p>Finally, run the agent with a task.</p> <pre><code>AIAgent.run(\"What is the weather in Virgina, New York and Washington DC?\")\n</code></pre> <pre><code>user:\nWhat is the weather in Virgina, New York and Washington DC?\n------------------------------------------------------------------------------\nassistant(tool_call):\nFunction name: GetWeather (Call Id: call_QOxUTdkWXhA5hlaKfEmvY3As)\nArguments: {\"location\": \"Virginia\"}\n--------------------------------------------------------------------------------\nassistant(tool_call):\nFunction name: GetWeather (Call Id: call_brouCb5MnCgPbK172dKaw0cx)\nArguments: {\"location\": \"New York\"}\n--------------------------------------------------------------------------------\nassistant(tool_call):\nFunction name: GetWeather (Call Id: call_KUH1ErAHdMFV83gYYghRdaIK)\nArguments: {\"location\": \"Washington DC\"}\n-------------------------------------------------------------------------------\ntool(Id: call_QOxUTdkWXhA5hlaKfEmvY3As):\nVirginia: 68F.\n-------------------------------------------------------------------------------\ntool(Id: call_brouCb5MnCgPbK172dKaw0cx):\nNew York: 62F.\n--------------------------------------------------------------------------------\ntool(Id: call_KUH1ErAHdMFV83gYYghRdaIK):\nWashington DC: 67F.\n--------------------------------------------------------------------------------\nassistant:\nHere is the current weather for each location:\n- **Virginia**: 68\u00b0F\n- **New York**: 62\u00b0F\n- **Washington DC**: 67\u00b0F\n-------------------------------------------------------------------------------\n</code></pre> <p>You can check the agent's <code>chat_history</code> property.</p> <pre><code>AIAgent.chat_history\n</code></pre> <pre><code>[{'role': 'user',\n  'content': 'What is the weather in Virgina, New York and Washington DC?'},\n {'content': 'Here is the current weather for each location:\\n\\n- **Virginia**: 68\u00b0F\\n- **New York**: 62\u00b0F\\n- **Washington DC**: 67\u00b0F',\n  'role': 'assistant'}]\n</code></pre> <p>You can also reset the agent's memory:</p> <pre><code>AIAgent.reset_memory()\n</code></pre>"},{"location":"home/quickstarts/dapr_workflows/","title":"Dapr &amp; Floki Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>Dapr workflows provide a solid framework for managing long-running processes and interactions across distributed systems using the Dapr Python SDK. Floki builds on this by introducing tasks, which simplify defining and managing workflows while adding features like tool integrations and LLM-powered reasoning. This approach allows you to start with basic Dapr workflows and expand to more advanced capabilities, such as LLM-driven tasks or multi-agent coordination, as your needs grow.</p>"},{"location":"home/quickstarts/dapr_workflows/#default-dapr-workflows","title":"Default Dapr Workflows","text":"<p>Creating a Dapr workflow is straightforward. Start by creating a python script <code>wf_taskchain_original_activity.py</code> and initializing the <code>WorkflowRuntime</code>, which manages the execution of workflows.</p> <pre><code>import dapr.ext.workflow as wf\n\nwfr = wf.WorkflowRuntime()\n</code></pre> <p>Next, define the workflow logic and the individual tasks it will execute. In this example, we pass a number through a sequence of steps, where each step performs an operation on the input. This follows the Dapr Task chaining pattern commonly used in Dapr workflows.</p> <pre><code>@wfr.workflow(name='random_workflow')\ndef task_chain_workflow(ctx: wf.DaprWorkflowContext, wf_input: int):\n    result1 = yield ctx.call_activity(step1, input=wf_input)\n    result2 = yield ctx.call_activity(step2, input=result1)\n    result3 = yield ctx.call_activity(step3, input=result2)\n    return [result1, result2, result3]\n\n@wfr.activity\ndef step1(ctx, activity_input):\n    print(f'Step 1: Received input: {activity_input}.')\n    # Do some work\n    return activity_input + 1\n\n@wfr.activity\ndef step2(ctx, activity_input):\n    print(f'Step 2: Received input: {activity_input}.')\n    # Do some work\n    return activity_input * 2\n\n@wfr.activity\ndef step3(ctx, activity_input):\n    print(f'Step 3: Received input: {activity_input}.')\n    # Do some work\n    return activity_input ^ 2\n</code></pre> <p>Finally, start the <code>WorkflowRuntime</code>, create a <code>DaprWorkflowClient</code>, and schedule the workflow. The rest of the script monitors the workflow's progress and handles its completion.</p> <pre><code>from time import sleep\n\nif __name__ == '__main__':\n    wfr.start()\n    sleep(5)  # wait for workflow runtime to start\n\n    wf_client = wf.DaprWorkflowClient()\n    instance_id = wf_client.schedule_new_workflow(workflow=task_chain_workflow, input=10)\n    print(f'Workflow started. Instance ID: {instance_id}')\n    state = wf_client.wait_for_workflow_completion(instance_id)\n    print(f'Workflow completed! Status: {state.runtime_status}')\n\n    wfr.shutdown()\n</code></pre> <p>With this setup, you can easily implement and execute workflows using Dapr, leveraging its powerful runtime and task orchestration capabilities.</p>"},{"location":"home/quickstarts/dapr_workflows/#set-up-a-state-store-for-workflows","title":"Set Up a State Store for Workflows","text":"<p>Before running a workflow, you need to define a Dapr component for the state store. Workflows in Dapr leverage the actor model under the hood, which requires a state store to manage actor states and ensure reliability. When running locally, you can use the default Redis state store that was set up during <code>dapr init</code>. To configure the state store for workflows, we add an <code>actorStateStore</code> definition, ensuring workflows have the persistence they need for fault tolerance and long-running operations.</p> <p>Info</p> <p>Create a folder named <code>components</code> and create a <code>workflowstate.yaml</code> file inside with the following content. Name the state store <code>workflowstatestore</code>.</p> <pre><code>apiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: workflowstatestore\nspec:\n  type: state.redis\n  version: v1\n  initTimeout: 1m\n  metadata:\n  - name: redisHost\n    value: localhost:6379\n  - name: redisPassword\n    value: \"\"\n  - name: actorStateStore\n    value: \"true\"\n</code></pre>"},{"location":"home/quickstarts/dapr_workflows/#run-workflow","title":"Run Workflow!","text":"<p>Now, you can run that workflow with the <code>Dapr CLI</code>:</p> <pre><code>dapr run --app-id originalwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_original_activity.py\n</code></pre> <p></p>"},{"location":"home/quickstarts/dapr_workflows/#dapr-workflow-floki-workflows","title":"Dapr Workflow -&gt; Floki Workflows","text":"<p>With <code>Floki</code>, the goal was to simplify workflows while adding flexibility and powerful integrations. I wanted to create a way to track the workflow state, including input, output, and status, while also streamlining monitoring. To achieve this, I built additional <code>workflow</code> and <code>activity</code> wrappers. The workflow wrapper stays mostly the same as Dapr's original, but the activity wrapper has been extended into a <code>task wrapper</code>. This change allows tasks to integrate seamlessly with LLM-based prompts and other advanced capabilities.</p> <p>Info</p> <p>The same example as before can be written in the following way. While the difference might not be immediately noticeable, this is a straightforward example of task chaining using Python functions. Create a file named <code>wf_taskchain_floki_activity.py</code>.</p> <pre><code>from floki import WorkflowApp\nfrom floki.types import DaprWorkflowContext\n\nwfapp = WorkflowApp()\n\n@wfapp.workflow(name='random_workflow')\ndef task_chain_workflow(ctx:DaprWorkflowContext, input: int):\n    result1 = yield ctx.call_activity(step1, input=input)\n    result2 = yield ctx.call_activity(step2, input=result1)\n    result3 = yield ctx.call_activity(step3, input=result2)\n    return [result1, result2, result3]\n\n@wfapp.task\ndef step1(activity_input):\n    print(f'Step 1: Received input: {activity_input}.')\n    # Do some work\n    return activity_input + 1\n\n@wfapp.task\ndef step2(activity_input):\n    print(f'Step 2: Received input: {activity_input}.')\n    # Do some work\n    return activity_input * 2\n\n@wfapp.task\ndef step3(activity_input):\n    print(f'Step 3: Received input: {activity_input}.')\n    # Do some work\n    return activity_input ^ 2\n\nif __name__ == '__main__':\n    results = wfapp.run_and_monitor_workflow(task_chain_workflow, input=10)\n    print(f\"Results: {results}\")\n</code></pre> <p>Now, you can run that workflow with the same command with the <code>Dapr CLI</code>:</p> <pre><code>dapr run --app-id flokiwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_floki_activity.py\n</code></pre> <p></p> <p>If we inspect the <code>Workflow State</code> in the state store, you would see something like this:</p> <pre><code>{\n    \"instances\":{\n        \"a0d2de00818e4f0098318f0bed5fa1ee\":{\n            \"input\":\"10\",\n            \"output\":\"[11, 22, 20]\",\n            \"status\":\"completed\",\n            \"start_time\":\"2024-11-27T16:57:14.465235\",\n            \"end_time\":\"2024-11-27T16:57:17.540588\",\n            \"messages\":[]\n        }\n    }\n}\n</code></pre> <p><code>Floki</code> processes the workflow execution and even extracts the final output.</p>"},{"location":"home/quickstarts/llm/","title":"LLM Inference Client","text":"<p>In <code>Floki</code>, the LLM Inference Client is responsible for interacting with language models. It serves as the interface through which the agent communicates with the LLM, generating responses based on the input provided.</p> <p>Info</p> <p>By default, <code>Floki</code> uses the <code>OpenAIChatClient</code> to interact with the OpenAI Chat endpoint. By default, the <code>OpenAIChatClient</code> uses the <code>gpt-4o</code> model</p>"},{"location":"home/quickstarts/llm/#set-environment-variables","title":"Set Environment Variables","text":"<p>Create an <code>.env</code> file for your API keys and other environment variables with sensitive information that you do not want to hardcode.</p> <pre><code>OPENAI_API_KEY=\"XXXXXX\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Use Python-dotenv to load environment variables from <code>.env</code>.</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # take environment variables from .env.\n</code></pre>"},{"location":"home/quickstarts/llm/#basic-example","title":"Basic Example","text":"<p>By default, you can easily initialize the <code>OpenAIChatClient</code> without additional configuration. It uses the <code>OpenAI API</code> key from your environment variables.</p> <pre><code>from floki import OpenAIChatClient\n\nllm = OpenAIChatClient()\n\nllm.generate('Name a famous dog!')\n</code></pre> <p>This will generate a response using the <code>OpenAI</code> model, querying for the name of a famous dog.</p> <pre><code>ChatCompletion(choices=[Choice(finish_reason='stop', index=0, message=MessageContent(content='One famous dog is Lassie, the Rough Collie who became popular through films, television series, and books starting in the 1940s.', role='assistant'), logprobs=None)], created=1732713689, id='chatcmpl-AYCGvJxgP61OPy96Z3YW2dLAz7IJW', model='gpt-4o-2024-08-06', object='chat.completion', usage={'completion_tokens': 30, 'prompt_tokens': 12, 'total_tokens': 42, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}})\n</code></pre>"},{"location":"home/quickstarts/llm/#structured-output","title":"Structured Output","text":"<p>Onge again, initialize <code>OpenAIChatClient</code>.</p> <pre><code>from floki import OpenAIChatClient\n\nllmClient = OpenAIChatClient()\n</code></pre> <p>Define the response structure. You can use pydantic models.</p> <pre><code>from pydantic import BaseModel\n\nclass dog(BaseModel):\n    name: str\n    breed: str\n    reason: str\n</code></pre> <p>Finally, you can pass the response model to the LLM Client call.</p> <pre><code>from floki.types import UserMessage\n\nresponse = llmClient.generate(\n    messages=[UserMessage(\"One famous dog in history.\")],\n    response_model=dog\n)\nresponse\n</code></pre> <pre><code>dog(name='Hachiko', breed='Akita', reason=\"known for his remarkable loyalty to his owner, even many years after his owner's death\")\n</code></pre>"},{"location":"home/quickstarts/llm_workflows/","title":"LLM-based Task Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>In <code>Floki</code>, LLM-based Task Workflows allow developers to design step-by-step workflows where LLMs provide reasoning and decision-making at defined stages. These workflows are deterministic and structured, enabling the execution of tasks in a specific order, often defined by Python functions. This approach does not rely on event-driven systems or pub/sub messaging but focuses on defining and orchestrating tasks with the help of LLM reasoning when necessary. Ideal for scenarios that require a predefined flow of tasks enhanced by language model insights.</p> <p>Now that we have a better understanding of <code>Dapr</code> and <code>Floki</code> workflows, let\u2019s explore how to use Dapr activities or Floki tasks to call LLM Inference APIs, such as OpenAI Tex Generation endpoint, with models like <code>gpt-4o</code>.</p>"},{"location":"home/quickstarts/llm_workflows/#dapr-workflows-llm-inference-apis","title":"Dapr Workflows &amp; LLM Inference APIs","text":"<p>To start, we can define a few <code>Dapr</code> activities that interact with <code>OpenAI APIs</code>. These activities can be chained together so the output of one step becomes the input for the next. For example, in the first step, we can use the LLM\u2019s parameteric knowledge to pick a random character from The Lord of the Rings. In the second step, the LLM can generate a famous line spoken by that character.</p> <p>Tip</p> <p>Make sure you have your environment variables set up in an <code>.env</code> file so that the library can pick it up and use it to communicate with <code>OpenAI</code> services. We set them up in the LLM Inference Client section</p> <p>Start by initializing the <code>WorkflowRuntime</code> and gathering the right environment variables.</p> <pre><code>import dapr.ext.workflow as wf\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Initialize Workflow Instance\nwfr = wf.WorkflowRuntime()\n</code></pre> <p>Next, let's define a workflow and activities, demonstrating how each step integrates with the OpenAI Python SDK to achieve this functionality.</p> <pre><code># Define Workflow logic\n@wfr.workflow(name='lotr_workflow')\ndef task_chain_workflow(ctx: wf.DaprWorkflowContext):\n    result1 = yield ctx.call_activity(get_character)\n    result2 = yield ctx.call_activity(get_line, input=result1)\n    return result2\n\n# Activity 1\n@wfr.activity(name='step1')\ndef get_character(ctx):\n    client = OpenAI()\n    response = client.chat.completions.create(\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": \"Pick a random character from The Lord of the Rings and respond with the character name only\"\n            }\n        ],\n        model = 'gpt-4o'\n    )\n    character = response.choices[0].message.content\n    print(f\"Character: {character}\")\n    return character\n\n# Activity 2\n@wfr.activity(name='step2')\ndef get_line(ctx, character: str):\n    client = OpenAI()\n    response = client.chat.completions.create(\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"What is a famous line by {character}\"\n            }\n        ],\n        model = 'gpt-4o'\n    )\n    line = response.choices[0].message.content\n    print(f\"Line: {line}\")\n    return line\n</code></pre> <p>Finally, we complete the process by triggering the workflow and handling the output, including waiting for the workflow to finish before processing the results.</p> <pre><code>from time import sleep\n\nif __name__ == '__main__':\n    wfr.start()\n    sleep(5)  # wait for workflow runtime to start\n\n    wf_client = wf.DaprWorkflowClient()\n    instance_id = wf_client.schedule_new_workflow(workflow=task_chain_workflow)\n    print(f'Workflow started. Instance ID: {instance_id}')\n    state = wf_client.wait_for_workflow_completion(instance_id)\n    print(f'Workflow completed! Status: {state.runtime_status}')\n\n    wfr.shutdown()\n</code></pre> <p>Tip</p> <p>Before running a workflow, remember that you need to define a Dapr component for the state store.</p> <pre><code>dapr run --app-id originalllmwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_openai_original_llm_request.py\n</code></pre> <p></p>"},{"location":"home/quickstarts/llm_workflows/#floki-llm-based-tasks","title":"Floki LLM-based Tasks","text":"<p>Now, let\u2019s get to the exciting part! <code>Tasks</code> in <code>Floki</code> build on the concept of <code>activities</code> and bring additional flexibility. Using Python function signatures, you can define tasks with ease. The <code>task decorator</code> allows you to provide a <code>description</code> parameter, which acts as a prompt for the default LLM inference client in <code>Floki</code> (<code>OpenAIChatClient</code> by default).</p> <p>You can also use function arguments to pass variables to the prompt, letting you dynamically format the prompt before it\u2019s sent to the text generation endpoint. This makes it simple to implement workflows that follow the Dapr Task chaining pattern, just like in the earlier example, but with even more flexibility.</p> <pre><code>from floki import WorkflowApp\nfrom floki.types import DaprWorkflowContext\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Initialize the WorkflowApp\nwfapp = WorkflowApp()\n\n# Define Workflow logic\n@wfapp.workflow(name='lotr_workflow')\ndef task_chain_workflow(ctx: DaprWorkflowContext):\n    result1 = yield ctx.call_activity(get_character)\n    result2 = yield ctx.call_activity(get_line, input={\"character\": result1})\n    return result2\n\n@wfapp.task(description=\"\"\"\n    Pick a random character from The Lord of the Rings\\n\n    and respond with the character's name only\n\"\"\")\ndef get_character() -&gt; str:\n    pass\n\n@wfapp.task(description=\"What is a famous line by {character}\",)\ndef get_line(character: str) -&gt; str:\n    pass\n\nif __name__ == '__main__':\n    results = wfapp.run_and_monitor_workflow(task_chain_workflow)\n    print(f\"Famous Line: {results}\")\n</code></pre> <p>Run the workflow with the following command:</p> <pre><code>dapr run --app-id flokillmmwf --dapr-grpc-port 50001 --resources-path components/ -- python3 wf_taskchain_openai_floki_llm_request.py\n</code></pre> <p></p>"}]}